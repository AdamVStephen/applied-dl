{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b: hello-subclassing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "46IqQPIbHy5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST: Subclassing and GradientTape edition\n",
        "\n",
        "An example showing how to use Keras [Subclassing](https://www.tensorflow.org/guide/keras) in TensorFlow 2.0. We'll also use a [GradientTape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape) to write our training loop. \n",
        "\n",
        "You can find more details about this style, and how it compares to the previous one, in this [article](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021)."
      ]
    },
    {
      "metadata": {
        "id": "Bp8cvZHw-Lee",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the nightly build\n",
        "\n",
        "This is early stage, a few bugs and rough edges are to be expeced."
      ]
    },
    {
      "metadata": {
        "id": "d90of0YY2AwS",
        "colab_type": "code",
        "outputId": "3be42d98-b6d7-47b2-e492-6605e4557662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190130)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0.dev2019012800)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a0,>=1.13.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0a20190129)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TWS4rAhy2ZLA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KN9zsDk-2H3-",
        "colab_type": "code",
        "outputId": "bc85e1bd-4195-4404-9052-f8b885b962da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"You have version\", tf.__version__)\n",
        "assert tf.__version__ >= \"2.0\" # TensorFlow â‰¥ 2.0 required"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have version 2.0.0-dev20190130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M7Qx4Lni3mhh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.nn import relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLe-k62u7Od9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ]
    },
    {
      "metadata": {
        "id": "6UK75R0d34eP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GssK6Svd7QtE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "metadata": {
        "id": "1LyAl_xd2WQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Dataset will be cached locally after it's downloaded\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# These types are required for the operation we use later to compute loss.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvShJo97L2i9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch and shuffle the data"
      ]
    },
    {
      "metadata": {
        "id": "abmLlAjjKe9r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll use `tf.data` to batch up our dataset. \n",
        "\n",
        "* This is a bit of a low-level approach (batching is handled automatically by `model.fit`, which we're not using here). The TensorFlow team is working on a helpful [library](https://github.com/tensorflow/datasets) of built-in datasets, as well, to make this easier. It's really nice, but doesn't support 2.0 quite yet.\n",
        "\n",
        "Then, we'll shuffle it.\n",
        "\n",
        "* In the code below, you'll notice we have a `buffer_size` parameter. What is this, and why is it necessary? Datasets are streams. These can be potentially infinite (e.g., if you're reading images from a directory, and performing data augmentation). Since we can't shuffle a stream, we maintain a buffer in memory of `shuffle_size` elements, and randomize that. Since MNIST is tiny, we'll just keep a buffer of the entire dataset."
      ]
    },
    {
      "metadata": {
        "id": "UnOpC7wP2iJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffle_buffer = len(x_train)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuffle_buffer)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_uGXeW_7UOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a model\n",
        "\n",
        "Using this style feels like Object-Oriented Python + NumPy development. Initialize your layers in the constructor, then write your forward pass in the call method."
      ]
    },
    {
      "metadata": {
        "id": "Yskf-gkc3W00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Unroll the images into arrays\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = relu(x)\n",
        "    x = self.d2(x)\n",
        "    return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuews7VZ9bl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate loss\n"
      ]
    },
    {
      "metadata": {
        "id": "OQ96tjTX433W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z3Im20vF7-G0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n",
        "Here, we've used a [GradientTape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape), rather than the built-in `model.fit` method, to train our model. "
      ]
    },
    {
      "metadata": {
        "id": "A1pX0XLR5rte",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_on_batch(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)\n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ef6oJl49WQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A method to calculate accuracy\n",
        "\n",
        "This method takes the logits (the output of our model) and the labels, and returns an accuracy score. There are helper methods for this in the codebase, but I figured it'd be useful to show you how to do it from scratch. In your head, you can replace `tf.*` with `np.*` to understand what it's doing."
      ]
    },
    {
      "metadata": {
        "id": "hB6lmhlF5nOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  acc = tf.reduce_sum(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n",
        "  return acc * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xOlEOIKK8SuG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "In this section we will:\n",
        "* Initialize our model and optimizer. Note that both have internal state, so if you'd like to restart training from scratch, you should intialize both again.\n",
        "\n",
        "* Iterate over the dataset, grabbing batches of images and labels.\n",
        "\n",
        "* Call the model on each batch (our forward pass).\n",
        "\n",
        "* Use the training loop above to calculate loss, gradients, and update the weights (backward pass).\n",
        "\n",
        "After each epoch, we will print out the accuracy on the train and test sets. As discussed in class, obviously do this with validation data, not test."
      ]
    },
    {
      "metadata": {
        "id": "3diKQ01_5v5E",
        "colab_type": "code",
        "outputId": "3d8aa8b1-a564-4c80-8774-2fa4fbda713e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1545
        }
      },
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Epoch', epoch + 1)\n",
        "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "    loss_value = train_on_batch(model, images, labels)\n",
        "    step = optimizer.iterations.numpy() \n",
        "    if step % 100 == 0:\n",
        "      print('Step %d\\tLoss: %.4f' % (step, loss_value))\n",
        "  \n",
        "  print('Train accuracy %.2f' % calc_accuracy(model(x_train), y_train))\n",
        "  print('Test accuracy %.2f\\n' % calc_accuracy(model(x_test), y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Step 100\tLoss: 0.3558\n",
            "Step 200\tLoss: 0.2786\n",
            "Step 300\tLoss: 0.1871\n",
            "Step 400\tLoss: 0.1419\n",
            "Train accuracy 94.53\n",
            "Test accuracy 94.39\n",
            "\n",
            "Epoch 2\n",
            "Step 500\tLoss: 0.1715\n",
            "Step 600\tLoss: 0.1882\n",
            "Step 700\tLoss: 0.1802\n",
            "Step 800\tLoss: 0.1690\n",
            "Step 900\tLoss: 0.0944\n",
            "Train accuracy 96.26\n",
            "Test accuracy 95.82\n",
            "\n",
            "Epoch 3\n",
            "Step 1000\tLoss: 0.0755\n",
            "Step 1100\tLoss: 0.0961\n",
            "Step 1200\tLoss: 0.0965\n",
            "Step 1300\tLoss: 0.0736\n",
            "Step 1400\tLoss: 0.1139\n",
            "Train accuracy 97.30\n",
            "Test accuracy 96.49\n",
            "\n",
            "Epoch 4\n",
            "Step 1500\tLoss: 0.1151\n",
            "Step 1600\tLoss: 0.1948\n",
            "Step 1700\tLoss: 0.0993\n",
            "Step 1800\tLoss: 0.0791\n",
            "Train accuracy 97.91\n",
            "Test accuracy 96.92\n",
            "\n",
            "Epoch 5\n",
            "Step 1900\tLoss: 0.0682\n",
            "Step 2000\tLoss: 0.1072\n",
            "Step 2100\tLoss: 0.1451\n",
            "Step 2200\tLoss: 0.0727\n",
            "Step 2300\tLoss: 0.0696\n",
            "Train accuracy 98.29\n",
            "Test accuracy 97.13\n",
            "\n",
            "Epoch 6\n",
            "Step 2400\tLoss: 0.0415\n",
            "Step 2500\tLoss: 0.0399\n",
            "Step 2600\tLoss: 0.0572\n",
            "Step 2700\tLoss: 0.0474\n",
            "Step 2800\tLoss: 0.0438\n",
            "Train accuracy 98.59\n",
            "Test accuracy 97.26\n",
            "\n",
            "Epoch 7\n",
            "Step 2900\tLoss: 0.0640\n",
            "Step 3000\tLoss: 0.0394\n",
            "Step 3100\tLoss: 0.0716\n",
            "Step 3200\tLoss: 0.0727\n",
            "Train accuracy 98.82\n",
            "Test accuracy 97.38\n",
            "\n",
            "Epoch 8\n",
            "Step 3300\tLoss: 0.0412\n",
            "Step 3400\tLoss: 0.0383\n",
            "Step 3500\tLoss: 0.0201\n",
            "Step 3600\tLoss: 0.0376\n",
            "Step 3700\tLoss: 0.0251\n",
            "Train accuracy 99.01\n",
            "Test accuracy 97.45\n",
            "\n",
            "Epoch 9\n",
            "Step 3800\tLoss: 0.0284\n",
            "Step 3900\tLoss: 0.0761\n",
            "Step 4000\tLoss: 0.0761\n",
            "Step 4100\tLoss: 0.0317\n",
            "Step 4200\tLoss: 0.0124\n",
            "Train accuracy 99.16\n",
            "Test accuracy 97.48\n",
            "\n",
            "Epoch 10\n",
            "Step 4300\tLoss: 0.0448\n",
            "Step 4400\tLoss: 0.0455\n",
            "Step 4500\tLoss: 0.0328\n",
            "Step 4600\tLoss: 0.0305\n",
            "Train accuracy 99.28\n",
            "Test accuracy 97.48\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFRdwFfm3z2q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay! As a next step, you can play with the model to see if you can increase accuracy. There's a lot we can do to optimize runtime performance as well (for example, by using [tf.function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function) to compile Python code), but we can get into that down the road."
      ]
    }
  ]
}