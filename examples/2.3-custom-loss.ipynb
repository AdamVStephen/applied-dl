{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom-loss.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "46IqQPIbHy5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How to write a custom loss function in TensorFlow 2.0\n",
        "\n",
        "Hey everyone, I've gotten a couple of questions about writing custom loss functions, so here's a complete example, brought to you from a nice coffee shop in NYC.  This notebook shows you how to:\n",
        "\n",
        "* Write your own implementation of softmax.\n",
        "* Write your own implementation of cross entropy loss.\n",
        "* Compare your method against a built-in one: [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits).\n",
        "\n",
        "The above part is only a few lines of code. \n",
        "\n",
        "Note: this notebook is **not** an explainer of how softmax or cross entropy works, it just shows the mechanics of writing your own version. In this notebook, we'll also train a simple model on MNIST using our implementations (just so we have a complete example that runs end-to-end). "
      ]
    },
    {
      "metadata": {
        "id": "Bp8cvZHw-Lee",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the nightly build"
      ]
    },
    {
      "metadata": {
        "id": "d90of0YY2AwS",
        "colab_type": "code",
        "outputId": "6b9ff446-230d-4456-9db2-89510767359d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190203)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.1)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0.dev2019012800)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a0,>=1.13.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0a20190203)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TWS4rAhy2ZLA",
        "colab_type": "code",
        "outputId": "d14f0a43-dda6-4fd0-8e5c-12b3d0c44fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"You have version\", tf.__version__)\n",
        "assert tf.__version__ >= \"2.0\" # TensorFlow â‰¥ 2.0 required"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have version 2.0.0-dev20190203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M7Qx4Lni3mhh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.nn import relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6UK75R0d34eP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 128\n",
        "n_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LyAl_xd2WQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Types are needed later when calculating loss\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnOpC7wP2iJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffle_buffer = len(x_train)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuffle_buffer)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yskf-gkc3W00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(n_classes)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = relu(x)\n",
        "    x = self.d2(x)\n",
        "    return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuews7VZ9bl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1) Comparison point\n",
        "\n",
        "I figured it'd be helpful to compare our implementation against the helper method [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) so we can understand what it's doing. Let's unpack the name for starters.\n",
        "\n",
        "* ```sparse``` indicates that our labels are integer encoded (as opposed to one-hot). If your labels were in one-hot format, you would use ```softmax_cross_entropy_with_logits``` instead.\n",
        "\n",
        "* The next part of the name is ```softmax_cross_entropy_with_logits``` -- why are these grouped together? Softmax activation is commonly followed by cross entropy loss, these are group together for convenience. "
      ]
    },
    {
      "metadata": {
        "id": "OQ96tjTX433W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def built_in_loss(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtHE0OEKrNWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2) Our implementation of softmax followed by cross entropy loss\n",
        "\n",
        "Next, we'll write our own version of the above code. It's just the four line below. To debug or understand what this block is doing, you can break it up into smaller pieces, and print out the shapes and/or data as you go. Note:  you can also convert tensors to NumPy with ```.numpy()```."
      ]
    },
    {
      "metadata": {
        "id": "5E3rkKEsrPXC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def our_loss(logits, labels):\n",
        "  \n",
        "  # softmax part\n",
        "  sm = tf.math.exp(logits) / tf.reduce_sum(tf.math.exp(logits), axis=1, keepdims=True)\n",
        "  sm = tf.clip_by_value(sm, 1e-7, 1 - 1e-7)\n",
        "  \n",
        "  # loss part\n",
        "  labels = tf.one_hot(labels, n_classes, dtype=tf.double)\n",
        "  return tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(sm), axis=1))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwBCSgRPWbGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "You can choose which loss function to use below (ours, or the built-in one)."
      ]
    },
    {
      "metadata": {
        "id": "A1pX0XLR5rte",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_on_batch(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    # Forward pass\n",
        "    logits = model(images)\n",
        "    loss_one = built_in_loss(logits, labels)\n",
        "    loss_two = our_loss(logits, labels)    \n",
        "    \n",
        "  # Backward pass\n",
        "  # I'll use our implementation to update the gradients.\n",
        "  grads = tape.gradient(loss_two, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_one, loss_two"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hB6lmhlF5nOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A helper function to calculate accuracy.\n",
        "# You can play around with the shapes to see what's going on.\n",
        "def calc_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  acc = tf.reduce_sum(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n",
        "  return acc * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3diKQ01_5v5E",
        "colab_type": "code",
        "outputId": "bcacf43a-dbff-497a-b783-e8081d46da6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1615
        }
      },
      "cell_type": "code",
      "source": [
        "# Loop over the dataset, grab batchs, and train our model\n",
        "# As we go, verify the loss returned by our implementation is\n",
        "# the same as the built-in methods.\n",
        "model = MyModel()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"Epoch\", epoch + 1, \"\\n\")\n",
        "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "    loss_one, loss_two = train_on_batch(model, images, labels)\n",
        "    \n",
        "    # You can use something like this as a quick sanity check\n",
        "    tf.debugging.assert_near(loss_one, loss_two, atol=0.001, rtol=0.001)\n",
        "    \n",
        "    step = optimizer.iterations.numpy() \n",
        "    if step % 100 == 0:\n",
        "      print(\"Step\", step)\n",
        "      print(\"Built-in loss: %.4f, Our loss: %.4f\" % (loss_one.numpy(), loss_two.numpy()))\n",
        "      print(\"\")\n",
        "      \n",
        "  print('Train accuracy %.2f' % calc_accuracy(model(x_train), y_train))\n",
        "  print('Test accuracy %.2f\\n' % calc_accuracy(model(x_test), y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "\n",
            "Step 100\n",
            "Built-in loss: 0.3350, Our loss: 0.3350\n",
            "\n",
            "Step 200\n",
            "Built-in loss: 0.2593, Our loss: 0.2593\n",
            "\n",
            "Step 300\n",
            "Built-in loss: 0.3217, Our loss: 0.3217\n",
            "\n",
            "Step 400\n",
            "Built-in loss: 0.2138, Our loss: 0.2138\n",
            "\n",
            "Train accuracy 94.09\n",
            "Test accuracy 93.99\n",
            "\n",
            "Epoch 2 \n",
            "\n",
            "Step 500\n",
            "Built-in loss: 0.1589, Our loss: 0.1589\n",
            "\n",
            "Step 600\n",
            "Built-in loss: 0.1708, Our loss: 0.1708\n",
            "\n",
            "Step 700\n",
            "Built-in loss: 0.2407, Our loss: 0.2407\n",
            "\n",
            "Step 800\n",
            "Built-in loss: 0.2550, Our loss: 0.2550\n",
            "\n",
            "Step 900\n",
            "Built-in loss: 0.1880, Our loss: 0.1880\n",
            "\n",
            "Train accuracy 96.26\n",
            "Test accuracy 95.75\n",
            "\n",
            "Epoch 3 \n",
            "\n",
            "Step 1000\n",
            "Built-in loss: 0.2820, Our loss: 0.2820\n",
            "\n",
            "Step 1100\n",
            "Built-in loss: 0.0662, Our loss: 0.0662\n",
            "\n",
            "Step 1200\n",
            "Built-in loss: 0.1961, Our loss: 0.1961\n",
            "\n",
            "Step 1300\n",
            "Built-in loss: 0.1316, Our loss: 0.1316\n",
            "\n",
            "Step 1400\n",
            "Built-in loss: 0.1194, Our loss: 0.1194\n",
            "\n",
            "Train accuracy 97.22\n",
            "Test accuracy 96.52\n",
            "\n",
            "Epoch 4 \n",
            "\n",
            "Step 1500\n",
            "Built-in loss: 0.1720, Our loss: 0.1720\n",
            "\n",
            "Step 1600\n",
            "Built-in loss: 0.1083, Our loss: 0.1083\n",
            "\n",
            "Step 1700\n",
            "Built-in loss: 0.1143, Our loss: 0.1143\n",
            "\n",
            "Step 1800\n",
            "Built-in loss: 0.0851, Our loss: 0.0851\n",
            "\n",
            "Train accuracy 97.82\n",
            "Test accuracy 96.88\n",
            "\n",
            "Epoch 5 \n",
            "\n",
            "Step 1900\n",
            "Built-in loss: 0.0715, Our loss: 0.0715\n",
            "\n",
            "Step 2000\n",
            "Built-in loss: 0.0591, Our loss: 0.0591\n",
            "\n",
            "Step 2100\n",
            "Built-in loss: 0.0688, Our loss: 0.0688\n",
            "\n",
            "Step 2200\n",
            "Built-in loss: 0.0780, Our loss: 0.0780\n",
            "\n",
            "Step 2300\n",
            "Built-in loss: 0.0780, Our loss: 0.0780\n",
            "\n",
            "Train accuracy 98.19\n",
            "Test accuracy 97.06\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}